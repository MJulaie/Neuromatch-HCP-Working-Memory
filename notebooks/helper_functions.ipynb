{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgAZbE2U0tNv"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install nilearn matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oorh9fu70tNw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Necessary for visualization\n",
    "from nilearn import plotting, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pyjW79VA0tNw"
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Ge1OjvUH0tNw"
   },
   "source": [
    "# Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQikOvM00tNw"
   },
   "outputs": [],
   "source": [
    "# The download cells will store the data in nested directories starting here:\n",
    "HCP_DIR = \"../data\"\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "  os.mkdir(HCP_DIR)\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in sec\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated multiple times in each subject\n",
    "N_RUNS_REST = 4\n",
    "N_RUNS_TASK = 2\n",
    "\n",
    "# Time series data are organized by experiment, with each experiment\n",
    "# having an LR and RL (phase-encode direction) acquistion\n",
    "BOLD_NAMES = [\n",
    "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
    "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
    "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
    "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
    "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
    "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
    "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
    "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
    "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
    "]\n",
    "\n",
    "# You may want to limit the subjects used during code development.\n",
    "# This will use all subjects:\n",
    "subjects = range(N_SUBJECTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "tQoHbzhZ0tNx"
   },
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "TSmGZfoD0tNx"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_ImtqAk0tNx"
   },
   "outputs": [],
   "source": [
    "def get_image_ids(name):\n",
    "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
    "\n",
    "    Args:\n",
    "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    Returns:\n",
    "      run_ids (list of int) : Numeric ID for experiment image files\n",
    "\n",
    "  \"\"\"\n",
    "  run_ids = [\n",
    "             i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n",
    "             ]\n",
    "  if not run_ids:\n",
    "    raise ValueError(f\"Found no data for '{name}'\")\n",
    "  return run_ids\n",
    "\n",
    "\n",
    "def load_timeseries(subject, name, dir,\n",
    "                    runs=None, concat=True, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject.\n",
    "\n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    dir (str) : data directory\n",
    "    run (None or int or list of ints): 0-based run(s) of the task to load,\n",
    "      or None to load all runs.\n",
    "    concat (bool) : If True, concatenate multiple runs in time\n",
    "    remove_mean (bool) : If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_tp array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  # Get the list relative 0-based index of runs to use\n",
    "  if runs is None:\n",
    "    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n",
    "  elif isinstance(runs, int):\n",
    "    runs = [runs]\n",
    "\n",
    "  # Get the first (1-based) run id for this experiment\n",
    "  offset = get_image_ids(name)[0]\n",
    "\n",
    "  # Load each run's data\n",
    "  bold_data = [\n",
    "               load_single_timeseries(subject,\n",
    "                                      offset + run,\n",
    "                                      dir,\n",
    "                                      remove_mean) for run in runs\n",
    "               ]\n",
    "\n",
    "  # Optionally concatenate in time\n",
    "  if concat:\n",
    "    bold_data = np.concatenate(bold_data, axis=-1)\n",
    "\n",
    "  return bold_data\n",
    "\n",
    "\n",
    "def load_single_timeseries(subject, bold_run, dir, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "\n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    bold_run (int): 1-based run index, across all tasks\n",
    "    dir (str) : data directory\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_path = os.path.join(dir, \"subjects\", str(subject), \"timeseries\")\n",
    "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "  ts = np.load(os.path.join(bold_path, bold_file))\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "  return ts\n",
    "\n",
    "\n",
    "def load_evs(subject, name, condition, dir):\n",
    "  \"\"\"Load EV (explanatory variable) data for one task condition.\n",
    "\n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of task\n",
    "    condition (str) : Name of condition\n",
    "    dir (str) : data directory\n",
    "\n",
    "  Returns\n",
    "    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n",
    "      of the condition for each run.\n",
    "\n",
    "  \"\"\"\n",
    "  evs = []\n",
    "  for id in get_image_ids(name):\n",
    "    task_key = BOLD_NAMES[id - 1]\n",
    "    ev_file = os.path.join(dir, \"subjects\", str(subject), \"EVs\",\n",
    "                           task_key, f\"{condition}.txt\")\n",
    "    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "    evs.append(ev)\n",
    "  return evs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "ZyQjgo6g0tNy"
   },
   "source": [
    "## Task-based analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIdhKjGp0tNy"
   },
   "outputs": [],
   "source": [
    "def condition_frames(run_evs, skip=0):\n",
    "  \"\"\"Identify timepoints corresponding to a given condition in each run.\n",
    "\n",
    "  Args:\n",
    "    run_evs (list of dicts) : Onset and duration of the event, per run\n",
    "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "      for hemodynamic lag\n",
    "\n",
    "  Returns:\n",
    "    frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
    "\n",
    "  \"\"\"\n",
    "  frames_list = []\n",
    "  for ev in run_evs:\n",
    "\n",
    "    # Determine when trial starts, rounded down\n",
    "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "\n",
    "    # Use trial duration to determine how many frames to include for trial\n",
    "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "\n",
    "    # Take the range of frames that correspond to this specific trial\n",
    "    frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
    "\n",
    "    frames_list.append(np.concatenate(frames))\n",
    "\n",
    "  return frames_list\n",
    "\n",
    "\n",
    "def selective_average(timeseries_data, ev, skip=0):\n",
    "  \"\"\"Take the temporal mean across frames for a given condition.\n",
    "\n",
    "  Args:\n",
    "    timeseries_data (array or list of arrays): n_parcel x n_tp arrays\n",
    "    ev (dict or list of dicts): Condition timing information\n",
    "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "      for hemodynamic lag\n",
    "\n",
    "  Returns:\n",
    "    avg_data (1D array): Data averagted across selected image frames based\n",
    "    on condition timing\n",
    "\n",
    "  \"\"\"\n",
    "  # Ensure that we have lists of the same length\n",
    "  if not isinstance(timeseries_data, list):\n",
    "    timeseries_data = [timeseries_data]\n",
    "  if not isinstance(ev, list):\n",
    "    ev = [ev]\n",
    "  if len(timeseries_data) != len(ev):\n",
    "    raise ValueError(\"Length of `timeseries_data` and `ev` must match.\")\n",
    "\n",
    "  # Identify the indices of relevant frames\n",
    "  frames = condition_frames(ev, skip)\n",
    "\n",
    "  # Select the frames from each image\n",
    "  selected_data = []\n",
    "  for run_data, run_frames in zip(timeseries_data, frames):\n",
    "    run_frames = run_frames[run_frames < run_data.shape[1]]\n",
    "    selected_data.append(run_data[:, run_frames])\n",
    "\n",
    "  # Take the average in each parcel\n",
    "  avg_data = np.concatenate(selected_data, axis=-1).mean(axis=-1)\n",
    "\n",
    "  return avg_data"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZDFPnQ07MmEd",
    "qQzCA99sMryW"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
